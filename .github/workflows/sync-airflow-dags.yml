name: Sync Airflow DAGs to Remote Host

on:
  push:
    branches:
      - master
      - main
    paths:
      - 'lianel/dc/dags/**'
      - '.github/workflows/sync-airflow-dags.yml'
  workflow_dispatch:  # Allow manual trigger

# Optional: set these secrets to populate Airflow Variables for Comp-AI DAG (comp_ai_control_tests)
# COMP_AI_BASE_URL - default http://lianel-comp-ai-service:3002 (from same host/docker network)
# COMP_AI_TOKEN - Bearer token for Comp-AI API (Keycloak service account or user token)

env:
  REMOTE_HOST: ${{ secrets.REMOTE_HOST }}
  REMOTE_USER: ${{ secrets.REMOTE_USER }}
  REMOTE_PORT: ${{ secrets.REMOTE_PORT || '22' }}

jobs:
  sync-dags:
    name: Sync DAGs to Remote Host
    runs-on: ubuntu-latest
    permissions:
      contents: read

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Validate Secrets
        run: |
          if [ -z "${{ secrets.REMOTE_HOST }}" ]; then
            echo "‚ùå Error: REMOTE_HOST secret is not set"
            exit 1
          fi
          if [ -z "${{ secrets.REMOTE_USER }}" ]; then
            echo "‚ùå Error: REMOTE_USER secret is not set"
            exit 1
          fi
          if [ -z "${{ secrets.SSH_PRIVATE_KEY }}" ]; then
            echo "‚ùå Error: SSH_PRIVATE_KEY secret is not set"
            exit 1
          fi
          echo "‚úÖ All required secrets are configured"

      - name: Setup SSH Key
        env:
          REMOTE_HOST: ${{ secrets.REMOTE_HOST }}
          SSH_PRIVATE_KEY: ${{ secrets.SSH_PRIVATE_KEY }}
        run: |
          mkdir -p ~/.ssh
          chmod 700 ~/.ssh
          
          # Write SSH key - handle multiline properly
          printf '%s\n' "$SSH_PRIVATE_KEY" > ~/.ssh/deploy_key
          chmod 600 ~/.ssh/deploy_key
          
          # Verify key file exists and has content
          if [ ! -s ~/.ssh/deploy_key ]; then
            echo "‚ùå Error: SSH key file is empty or missing"
            exit 1
          fi
          
          # Verify key format
          if ! grep -q "^-----BEGIN" ~/.ssh/deploy_key; then
            echo "‚ùå Error: SSH key format is incorrect"
            echo "Key should start with -----BEGIN"
            exit 1
          fi
          
          # Check key size
          KEY_SIZE=$(wc -c < ~/.ssh/deploy_key)
          if [ "$KEY_SIZE" -lt 100 ]; then
            echo "‚ùå Error: SSH key file is too small ($KEY_SIZE bytes)"
            exit 1
          fi
          
          if [ -n "$REMOTE_HOST" ]; then
            ssh-keyscan -H "$REMOTE_HOST" >> ~/.ssh/known_hosts 2>/dev/null || true
          fi
          
          echo "‚úÖ SSH key ready (size: $KEY_SIZE bytes)"

      - name: Install rsync (if needed)
        run: |
          if ! command -v rsync &> /dev/null; then
            echo "Installing rsync..."
            sudo apt-get update -qq && sudo apt-get install -y rsync
          else
            echo "‚úÖ rsync is already installed"
          fi

      - name: Sync DAG Files
        env:
          REMOTE_HOST: ${{ secrets.REMOTE_HOST }}
          REMOTE_USER: ${{ secrets.REMOTE_USER }}
          REMOTE_PORT: ${{ secrets.REMOTE_PORT || '22' }}
        run: |
          echo "Syncing DAG files to remote host..."
          
          # Create DAGs directory if it doesn't exist
          ssh -i ~/.ssh/deploy_key \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=~/.ssh/known_hosts \
            -p ${REMOTE_PORT} \
            ${REMOTE_USER}@${REMOTE_HOST} \
            "mkdir -p /root/lianel/dc/dags && chmod 755 /root/lianel/dc/dags" || {
            echo "‚ùå Error: Failed to create DAGs directory on remote host"
            exit 1
          }
          
          # Sync all DAG files using rsync
          # Use -v for verbose, -z for compression, --delete to remove old files
          rsync -avz \
            -e "ssh -i ~/.ssh/deploy_key -o StrictHostKeyChecking=no -o UserKnownHostsFile=~/.ssh/known_hosts -p ${REMOTE_PORT}" \
            --delete \
            --exclude='*.pyc' \
            --exclude='__pycache__' \
            --exclude='.git' \
            lianel/dc/dags/ \
            ${REMOTE_USER}@${REMOTE_HOST}:/root/lianel/dc/dags/ || {
            echo "‚ùå Error: rsync failed"
            echo "Trying alternative method with scp..."
            # Fallback: use scp for individual files
            for file in lianel/dc/dags/*.py; do
              if [ -f "$file" ]; then
                scp -i ~/.ssh/deploy_key \
                  -o StrictHostKeyChecking=no \
                  -o UserKnownHostsFile=~/.ssh/known_hosts \
                  -P ${REMOTE_PORT} \
                  "$file" \
                  ${REMOTE_USER}@${REMOTE_HOST}:/root/lianel/dc/dags/ || {
                  echo "‚ö†Ô∏è  Warning: Failed to copy $file"
                }
              fi
            done
          }
          
          echo "‚úÖ DAG files synced successfully"

      - name: Verify DAG Files
        env:
          REMOTE_HOST: ${{ secrets.REMOTE_HOST }}
          REMOTE_USER: ${{ secrets.REMOTE_USER }}
          REMOTE_PORT: ${{ secrets.REMOTE_PORT || '22' }}
        run: |
          echo "Verifying DAG files on remote host..."
          
          ssh -i ~/.ssh/deploy_key \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=~/.ssh/known_hosts \
            -p ${REMOTE_PORT} \
            ${REMOTE_USER}@${REMOTE_HOST} \
            "ls -la /root/lianel/dc/dags/*.py | wc -l && echo 'DAG files found'"
          
          echo "‚úÖ Verification complete"

      - name: Set Airflow Variables for Comp-AI
        env:
          REMOTE_HOST: ${{ secrets.REMOTE_HOST }}
          REMOTE_USER: ${{ secrets.REMOTE_USER }}
          REMOTE_PORT: ${{ secrets.REMOTE_PORT || '22' }}
          COMP_AI_BASE_URL: ${{ secrets.COMP_AI_BASE_URL }}
          COMP_AI_TOKEN: ${{ secrets.COMP_AI_TOKEN }}
        run: |
          echo "Setting Comp-AI Airflow Variables (if COMP_AI_TOKEN is set)..."
          if [ -z "$COMP_AI_TOKEN" ]; then
            echo "‚ö†Ô∏è  COMP_AI_TOKEN not set ‚Äî skipping variable set. Add COMP_AI_TOKEN (and optionally COMP_AI_BASE_URL) as repo secrets to enable comp_ai_control_tests DAG."
            exit 0
          fi
          COMP_AI_BASE_URL="${COMP_AI_BASE_URL:-http://lianel-comp-ai-service:3002}"
          # Strip non-ASCII so Airflow/requests don't get 401 or header errors (e.g. ellipsis ‚Ä¶ in secret)
          COMP_AI_TOKEN_SAFE=$(printf '%s' "$COMP_AI_TOKEN" | tr -cd '[\x20-\x7E]')
          # Pass COMP_AI_* from runner to remote; guard against stopped containers.
          ssh -i ~/.ssh/deploy_key \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=~/.ssh/known_hosts \
            -p "${REMOTE_PORT}" \
            "${REMOTE_USER}@${REMOTE_HOST}" \
            "CONTAINER=\$(docker ps -aq -f name=airflow-apiserver | head -1); if [ -z \"\$CONTAINER\" ]; then echo 'No airflow-apiserver container found'; exit 0; fi; RUNNING=\$(docker inspect -f '{{.State.Running}}' \"\$CONTAINER\" 2>/dev/null || echo false); if [ \"\$RUNNING\" != 'true' ]; then echo 'airflow-apiserver exists but is not running; skipping variable set'; exit 0; fi; docker exec \"\$CONTAINER\" airflow variables set COMP_AI_BASE_URL '${COMP_AI_BASE_URL}' || exit 0; docker exec \"\$CONTAINER\" airflow variables set COMP_AI_TOKEN '${COMP_AI_TOKEN_SAFE}' || exit 0"
          echo "‚úÖ COMP_AI_BASE_URL and COMP_AI_TOKEN set in Airflow"

      - name: Set Comp-AI client_credentials Variables (optional)
        env:
          REMOTE_HOST: ${{ secrets.REMOTE_HOST }}
          REMOTE_USER: ${{ secrets.REMOTE_USER }}
          REMOTE_PORT: ${{ secrets.REMOTE_PORT || '22' }}
          COMP_AI_KEYCLOAK_URL: ${{ secrets.COMP_AI_KEYCLOAK_URL }}
          COMP_AI_CLIENT_ID: ${{ secrets.COMP_AI_CLIENT_ID }}
          COMP_AI_CLIENT_SECRET: ${{ secrets.COMP_AI_CLIENT_SECRET }}
          COMP_AI_KEYCLOAK_REALM: ${{ secrets.COMP_AI_KEYCLOAK_REALM }}
        run: |
          if [ -z "$COMP_AI_KEYCLOAK_URL" ] || [ -z "$COMP_AI_CLIENT_ID" ] || [ -z "$COMP_AI_CLIENT_SECRET" ]; then
            echo "‚ÑπÔ∏è  COMP_AI_KEYCLOAK_URL / COMP_AI_CLIENT_ID / COMP_AI_CLIENT_SECRET not all set ‚Äî skipping. Add these repo secrets to enable token refresh in DAGs (avoids 401 from expired token)."
            exit 0
          fi
          COMP_AI_KEYCLOAK_REALM="${COMP_AI_KEYCLOAK_REALM:-lianel}"
          COMP_AI_CLIENT_SECRET_SAFE=$(printf '%s' "$COMP_AI_CLIENT_SECRET" | tr -cd '[\x20-\x7E]')
          ssh -i ~/.ssh/deploy_key \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=~/.ssh/known_hosts \
            -p "${REMOTE_PORT}" \
            "${REMOTE_USER}@${REMOTE_HOST}" \
            "CONTAINER=\$(docker ps -aq -f name=airflow-apiserver | head -1); if [ -z \"\$CONTAINER\" ]; then exit 0; fi; RUNNING=\$(docker inspect -f '{{.State.Running}}' \"\$CONTAINER\" 2>/dev/null || echo false); if [ \"\$RUNNING\" != 'true' ]; then echo 'airflow-apiserver not running; skipping client_credentials variable set'; exit 0; fi; docker exec \"\$CONTAINER\" airflow variables set COMP_AI_KEYCLOAK_URL '${COMP_AI_KEYCLOAK_URL}' || exit 0; docker exec \"\$CONTAINER\" airflow variables set COMP_AI_CLIENT_ID '${COMP_AI_CLIENT_ID}' || exit 0; docker exec \"\$CONTAINER\" airflow variables set COMP_AI_CLIENT_SECRET '${COMP_AI_CLIENT_SECRET_SAFE}' || exit 0; docker exec \"\$CONTAINER\" airflow variables set COMP_AI_KEYCLOAK_REALM '${COMP_AI_KEYCLOAK_REALM}' || exit 0"
          echo "‚úÖ COMP_AI_KEYCLOAK_* and COMP_AI_CLIENT_* set in Airflow (DAGs will use client_credentials)"

      - name: Sync Summary
        run: |
          echo "## üîÑ Airflow DAGs Synced" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Source**: Repository DAGs directory" >> $GITHUB_STEP_SUMMARY
          echo "- **Destination**: \`${{ secrets.REMOTE_HOST }}:/root/lianel/dc/dags/\`" >> $GITHUB_STEP_SUMMARY
          echo "- **Status**: ‚úÖ Success" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "DAG files will be automatically detected by Airflow scheduler." >> $GITHUB_STEP_SUMMARY
