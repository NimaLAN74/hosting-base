{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ML Feature Analysis\n",
        "\n",
        "**Purpose**: Analyze ML dataset features, feature importance, distributions, and relationships\n",
        "\n",
        "**Date**: January 12, 2026\n",
        "\n",
        "## Objectives\n",
        "1. Analyze feature distributions and statistics\n",
        "2. Calculate feature importance and correlations\n",
        "3. Identify feature relationships and interactions\n",
        "4. Assess feature quality for ML models\n",
        "5. Visualize feature patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sqlalchemy import create_engine\n",
        "from scipy import stats\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Database connection\n",
        "DB_CONFIG = {\n",
        "    'host': '172.18.0.1',\n",
        "    'port': 5432,\n",
        "    'database': 'lianel_energy',\n",
        "    'user': 'airflow',\n",
        "    'password': 'P9xK2mN7vQ4wR8tY3sL6hJ5nB1cV0zX'\n",
        "}\n",
        "\n",
        "connection_string = f\"postgresql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}\"\n",
        "engine = create_engine(connection_string)\n",
        "\n",
        "print(\"âœ… Database connection established\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and Explore ML Dataset Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load all features from ML forecasting dataset\n",
        "query = \"\"\"\n",
        "SELECT \n",
        "    cntr_code,\n",
        "    year,\n",
        "    -- Target variables\n",
        "    total_energy_gwh,\n",
        "    renewable_energy_gwh,\n",
        "    fossil_energy_gwh,\n",
        "    -- Time features\n",
        "    year_index,\n",
        "    is_first_year,\n",
        "    is_last_year,\n",
        "    -- Lagged features\n",
        "    lag_1_year_total_energy_gwh,\n",
        "    lag_2_year_total_energy_gwh,\n",
        "    lag_3_year_total_energy_gwh,\n",
        "    lag_1_year_renewable_gwh,\n",
        "    lag_2_year_renewable_gwh,\n",
        "    -- YoY changes\n",
        "    yoy_change_total_energy_pct,\n",
        "    yoy_change_renewable_pct,\n",
        "    yoy_change_absolute_gwh,\n",
        "    -- Rolling statistics\n",
        "    rolling_3y_mean_total_energy_gwh,\n",
        "    rolling_5y_mean_total_energy_gwh,\n",
        "    rolling_3y_mean_renewable_gwh,\n",
        "    rolling_5y_mean_renewable_gwh,\n",
        "    -- Trend indicators\n",
        "    trend_3y_slope,\n",
        "    trend_5y_slope,\n",
        "    is_increasing_trend,\n",
        "    is_decreasing_trend,\n",
        "    -- Percentages\n",
        "    pct_renewable,\n",
        "    pct_fossil,\n",
        "    -- Spatial features\n",
        "    area_km2,\n",
        "    energy_density_gwh_per_km2,\n",
        "    feature_count\n",
        "FROM ml_dataset_forecasting_v1\n",
        "WHERE year >= 2018  -- Filter incomplete years\n",
        "ORDER BY cntr_code, year\n",
        "\"\"\"\n",
        "\n",
        "df = pd.read_sql(query, engine)\n",
        "print(f\"âœ… Loaded {len(df)} records\")\n",
        "print(f\"Features: {len(df.columns)}\")\n",
        "print(f\"\\nFeature list:\")\n",
        "for i, col in enumerate(df.columns, 1):\n",
        "    print(f\"  {i:2d}. {col}\")\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Feature Statistics and Distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select numeric features for analysis\n",
        "numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "# Remove country code and year (not features)\n",
        "numeric_features = [f for f in numeric_features if f not in ['year', 'year_index']]\n",
        "\n",
        "print(f\"ðŸ“Š Analyzing {len(numeric_features)} numeric features\")\n",
        "\n",
        "# Calculate comprehensive statistics\n",
        "feature_stats = df[numeric_features].describe().T\n",
        "feature_stats['missing_count'] = df[numeric_features].isnull().sum()\n",
        "feature_stats['missing_pct'] = (feature_stats['missing_count'] / len(df)) * 100\n",
        "feature_stats['zero_count'] = [((df[col] == 0).sum() if df[col].dtype in [np.int64, np.float64] else 0) for col in numeric_features]\n",
        "feature_stats['skewness'] = [stats.skew(df[col].dropna()) for col in numeric_features]\n",
        "feature_stats['kurtosis'] = [stats.kurtosis(df[col].dropna()) for col in numeric_features]\n",
        "\n",
        "print(\"\\nðŸ“ˆ Feature Statistics Summary:\")\n",
        "print(feature_stats[['count', 'mean', 'std', 'min', 'max', 'missing_pct', 'skewness']].round(2).head(20))\n",
        "\n",
        "# Visualize feature distributions\n",
        "n_features = len(numeric_features)\n",
        "n_cols = 4\n",
        "n_rows = (n_features + n_cols - 1) // n_cols\n",
        "\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5*n_rows))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, feature in enumerate(numeric_features[:min(20, len(numeric_features))]):\n",
        "    ax = axes[i]\n",
        "    data = df[feature].dropna()\n",
        "    if len(data) > 0:\n",
        "        ax.hist(data, bins=30, alpha=0.7, edgecolor='black')\n",
        "        ax.set_title(f'{feature}\\n(mean={data.mean():.2f})', fontsize=9)\n",
        "        ax.set_xlabel('Value')\n",
        "        ax.set_ylabel('Frequency')\n",
        "        ax.grid(True, alpha=0.3)\n",
        "    else:\n",
        "        ax.text(0.5, 0.5, 'No data', ha='center', va='center')\n",
        "        ax.set_title(feature, fontsize=9)\n",
        "\n",
        "# Hide unused subplots\n",
        "for i in range(len(numeric_features), len(axes)):\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Feature Correlations and Relationships"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate correlation matrix for key features\n",
        "key_features = [\n",
        "    'total_energy_gwh', 'renewable_energy_gwh', 'fossil_energy_gwh',\n",
        "    'pct_renewable', 'pct_fossil',\n",
        "    'lag_1_year_total_energy_gwh', 'lag_2_year_total_energy_gwh',\n",
        "    'yoy_change_total_energy_pct', 'yoy_change_renewable_pct',\n",
        "    'rolling_3y_mean_total_energy_gwh', 'rolling_5y_mean_total_energy_gwh',\n",
        "    'trend_3y_slope', 'trend_5y_slope',\n",
        "    'energy_density_gwh_per_km2', 'area_km2'\n",
        "]\n",
        "\n",
        "# Filter to features that exist\n",
        "key_features = [f for f in key_features if f in df.columns]\n",
        "\n",
        "corr_matrix = df[key_features].corr()\n",
        "\n",
        "# Visualize correlation matrix\n",
        "plt.figure(figsize=(14, 12))\n",
        "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
        "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8}, \n",
        "            xticklabels=True, yticklabels=True, fontsize=8)\n",
        "plt.title('Feature Correlation Matrix', fontsize=14, fontweight='bold')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find highly correlated features\n",
        "high_corr_pairs = []\n",
        "for i in range(len(corr_matrix.columns)):\n",
        "    for j in range(i+1, len(corr_matrix.columns)):\n",
        "        corr_val = corr_matrix.iloc[i, j]\n",
        "        if abs(corr_val) > 0.8:\n",
        "            high_corr_pairs.append((\n",
        "                corr_matrix.columns[i],\n",
        "                corr_matrix.columns[j],\n",
        "                corr_val\n",
        "            ))\n",
        "\n",
        "print(\"\\nðŸ”— Highly Correlated Feature Pairs (|r| > 0.8):\")\n",
        "if high_corr_pairs:\n",
        "    for feat1, feat2, corr in sorted(high_corr_pairs, key=lambda x: abs(x[2]), reverse=True):\n",
        "        print(f\"  {feat1} <-> {feat2}: {corr:.3f}\")\n",
        "else:\n",
        "    print(\"  No highly correlated pairs found\")\n",
        "\n",
        "# Correlation with target variable\n",
        "target = 'total_energy_gwh'\n",
        "if target in df.columns:\n",
        "    target_corr = df[numeric_features].corrwith(df[target]).sort_values(ascending=False)\n",
        "    print(f\"\\nðŸŽ¯ Correlation with Target ({target}):\")\n",
        "    print(target_corr.head(15).to_string())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Feature Importance Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple feature importance using variance and correlation with target\n",
        "target = 'total_energy_gwh'\n",
        "\n",
        "if target in df.columns:\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': numeric_features,\n",
        "        'variance': [df[col].var() if df[col].dtype in [np.int64, np.float64] else 0 for col in numeric_features],\n",
        "        'correlation_with_target': [df[col].corr(df[target]) if col != target else 1.0 for col in numeric_features],\n",
        "        'mean_abs_value': [df[col].abs().mean() if df[col].dtype in [np.int64, np.float64] else 0 for col in numeric_features]\n",
        "    })\n",
        "    \n",
        "    # Normalize for importance score\n",
        "    feature_importance['variance_norm'] = (feature_importance['variance'] - feature_importance['variance'].min()) / (feature_importance['variance'].max() - feature_importance['variance'].min() + 1e-10)\n",
        "    feature_importance['corr_norm'] = feature_importance['correlation_with_target'].abs()\n",
        "    feature_importance['importance_score'] = (\n",
        "        feature_importance['variance_norm'] * 0.3 +\n",
        "        feature_importance['corr_norm'] * 0.7\n",
        "    )\n",
        "    \n",
        "    feature_importance = feature_importance.sort_values('importance_score', ascending=False)\n",
        "    \n",
        "    print(\"ðŸ“Š Feature Importance Ranking:\")\n",
        "    print(feature_importance[['feature', 'correlation_with_target', 'variance', 'importance_score']].head(20).to_string(index=False))\n",
        "    \n",
        "    # Visualize top features\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "    \n",
        "    # Plot 1: Top features by importance\n",
        "    ax1 = axes[0]\n",
        "    top_features = feature_importance.head(15)\n",
        "    ax1.barh(top_features['feature'], top_features['importance_score'], color='steelblue', alpha=0.7)\n",
        "    ax1.set_xlabel('Importance Score')\n",
        "    ax1.set_title('Top 15 Features by Importance Score')\n",
        "    ax1.grid(True, alpha=0.3, axis='x')\n",
        "    \n",
        "    # Plot 2: Correlation with target\n",
        "    ax2 = axes[1]\n",
        "    top_corr = feature_importance.nlargest(15, 'correlation_with_target')\n",
        "    colors = ['green' if x > 0 else 'red' for x in top_corr['correlation_with_target']]\n",
        "    ax2.barh(top_corr['feature'], top_corr['correlation_with_target'], color=colors, alpha=0.7)\n",
        "    ax2.set_xlabel('Correlation with Target')\n",
        "    ax2.set_title('Top 15 Features by Correlation with Target')\n",
        "    ax2.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
        "    ax2.grid(True, alpha=0.3, axis='x')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Feature Quality Assessment\n",
        "\n",
        "### Summary\n",
        "- Feature distributions and statistics analyzed\n",
        "- Correlations and relationships identified\n",
        "- Feature importance calculated\n",
        "- Ready for ML model training"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
