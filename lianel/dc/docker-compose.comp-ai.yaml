# Comp-AI stack. For same-host local model experiment, set COMP_AI_OLLAMA_URL and
# COMP_AI_OLLAMA_MODEL in .env and start the ollama service; then pull a model on the remote host.
services:
  comp-ai-service:
    image: ghcr.io/nimalan74/hosting-base/lianel-comp-ai-service:latest
    container_name: lianel-comp-ai-service
    env_file:
      - .env
    environment:
      PORT: 3002
      POSTGRES_HOST: ${POSTGRES_HOST:-172.18.0.1}
      POSTGRES_PORT: ${POSTGRES_PORT:-5432}
      POSTGRES_USER: ${POSTGRES_USER:-airflow}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB:-lianel_energy}
      KEYCLOAK_URL: https://auth.lianel.se
      KEYCLOAK_REALM: ${KEYCLOAK_REALM:-lianel}
      KEYCLOAK_ADMIN_USER: ${KEYCLOAK_ADMIN_USER}
      KEYCLOAK_ADMIN_PASSWORD: ${KEYCLOAK_ADMIN_PASSWORD}
      COMP_AI_KEYCLOAK_CLIENT_ID: ${COMP_AI_KEYCLOAK_CLIENT_ID:-comp-ai-service}
      COMP_AI_KEYCLOAK_CLIENT_SECRET: ${COMP_AI_KEYCLOAK_CLIENT_SECRET}
      COMP_AI_API_KEY: ${COMP_AI_API_KEY:-}
      COMP_AI_MODEL_PATH: ${COMP_AI_MODEL_PATH:-/models}
      COMP_AI_OLLAMA_URL: ${COMP_AI_OLLAMA_URL:-http://ollama:11434}
      COMP_AI_OLLAMA_MODEL: ${COMP_AI_OLLAMA_MODEL:-tinyllama}
      COMP_AI_OLLAMA_FALLBACK_TO_MOCK: ${COMP_AI_OLLAMA_FALLBACK_TO_MOCK:-true}
      COMP_AI_MAX_TOKENS: ${COMP_AI_MAX_TOKENS:-4096}
      COMP_AI_TEMPERATURE: ${COMP_AI_TEMPERATURE:-0.7}
      COMP_AI_RATE_LIMIT_REQUESTS: ${COMP_AI_RATE_LIMIT_REQUESTS:-60}
      COMP_AI_RATE_LIMIT_WINDOW_SECS: ${COMP_AI_RATE_LIMIT_WINDOW_SECS:-60}
      COMP_AI_MAX_PROMPT_LEN: ${COMP_AI_MAX_PROMPT_LEN:-32768}
      COMP_AI_RESPONSE_CACHE_TTL_SECS: ${COMP_AI_RESPONSE_CACHE_TTL_SECS:-300}
      COMP_AI_RESPONSE_CACHE_MAX_ENTRIES: ${COMP_AI_RESPONSE_CACHE_MAX_ENTRIES:-1000}
      GITHUB_TOKEN: ${GITHUB_TOKEN:-}
      RUST_LOG: info
    expose:
      - "3002"
    depends_on:
      - keycloak
    restart: unless-stopped
    networks:
      - lianel-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3002/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Optional: same-host local model (Ollama). Enable by setting COMP_AI_OLLAMA_URL=http://ollama:11434
  # and COMP_AI_OLLAMA_MODEL=<name> in .env, then start this service and pull a model on the remote host.
  ollama:
    image: ollama/ollama:latest
    container_name: lianel-ollama
    expose:
      - "11434"
    volumes:
      - ollama-data:/root/.ollama
    restart: unless-stopped
    networks:
      - lianel-network
    profiles:
      - local-model

networks:
  lianel-network:
    external: true

volumes:
  ollama-data:
